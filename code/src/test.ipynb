{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y3/qz4wgjts6dj2m3kdg2nq67200000gn/T/ipykernel_24476/1592899246.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chufeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/chufeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/chufeng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/chufeng/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Loading Word2Vec Model: 100%|██████████| 1/1 [00:17<00:00, 17.00s/model]\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chufeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/chufeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/chufeng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/chufeng/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Loading Word2Vec Model: 100%|██████████| 1/1 [00:17<00:00, 17.37s/model]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from modules import preprocessing as pre\n",
    "from modules import extract_keywords as ek\n",
    "from modules import visualise as vis\n",
    "import models.ovr_model as ovr\n",
    "import hyperparm as hpt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim.downloader\n",
    "from tqdm import tqdm\n",
    "\n",
    "MIN_COSINE_VALUE = 0.28\n",
    "\n",
    "def load_word2vec_model():\n",
    "    with tqdm(total=1, desc=\"Loading Word2Vec Model\", unit=\"model\") as pbar:\n",
    "        w2v_model = gensim.downloader.load('word2vec-google-news-300')\n",
    "        pbar.update(1)\n",
    "    return w2v_model\n",
    "\n",
    "w2v = load_word2vec_model()\n",
    "\n",
    "def merge_lists(l1, min_cosine_value, w2v, ref_list):\n",
    "    final_list = []\n",
    "    \n",
    "    for word in l1:\n",
    "        if word in w2v:\n",
    "            similarities = []\n",
    "            for ref_word in ref_list:\n",
    "                if ref_word in w2v:\n",
    "                    sim = w2v.similarity(word, ref_word)\n",
    "                    similarities.append(sim)\n",
    "            if any(sim >= min_cosine_value for sim in similarities):\n",
    "                final_list.append(word)\n",
    "    if 'use' in final_list: final_list.remove('use')\n",
    "    elif 'also' in final_list: final_list.remove('also')\n",
    "    return final_list\n",
    "\n",
    "def better_keywords(df):\n",
    "    for i in range(len(df)):\n",
    "        if df.iloc[i]['STRIDE'] == 0:\n",
    "            updated_list = merge_lists(df.loc[i, 'NameDesc'], MIN_COSINE_VALUE, w2v, S_final)\n",
    "        elif df.iloc[i]['STRIDE'] == 1:\n",
    "            updated_list = merge_lists(df.loc[i, 'NameDesc'], MIN_COSINE_VALUE, w2v, E_final)\n",
    "        elif df.iloc[i]['STRIDE'] == 2:\n",
    "            updated_list = merge_lists(df.loc[i, 'NameDesc'], MIN_COSINE_VALUE, w2v, D_final)\n",
    "        elif df.iloc[i]['STRIDE'] == 3:\n",
    "            updated_list = merge_lists(df.loc[i, 'NameDesc'], MIN_COSINE_VALUE, w2v, I_final)\n",
    "        elif df.iloc[i]['STRIDE'] == 4:\n",
    "            updated_list = merge_lists(df.loc[i, 'NameDesc'], MIN_COSINE_VALUE, w2v, R_final)\n",
    "        else:\n",
    "            updated_list = merge_lists(df.loc[i, 'NameDesc'], MIN_COSINE_VALUE, w2v, T_final)\n",
    "        \n",
    "        df.at[i, 'NameDesc'] = updated_list\n",
    "    return df\n",
    "\n",
    "def basic_processing(df):\n",
    "    words_to_remove = [\"e.g.\", \"code\", \"may\", \"attack\", \"system\", \"adversary\", \"Adversaries\"]\n",
    "    for word in words_to_remove:\n",
    "        df['NameDesc'] = df['NameDesc'].apply(lambda x: x.replace(word, ''))\n",
    "    for word in words_to_remove:\n",
    "        df['NameDesc'] = df['NameDesc'].apply(lambda x: re.sub(r'\\b' + re.escape(word) + r'\\b', '', x))\n",
    "\n",
    "    # df['NameDesc'] = df['NameDesc'].str.replace(r\"\\b(\" + \"|\".join(words_to_remove) + r\")\\b\", \"\", regex=True)\n",
    "    df['NameDesc'] = df['NameDesc'].str.replace(\"<br><br>\", \"\", regex=True)\n",
    "    df['NameDesc'] = df['NameDesc'].str.replace(\"\\(Citation:.*?\\)\", \"\", regex=True)\n",
    "    df['NameDesc'] = df['NameDesc'].str.replace(\"http\\S+\", \"\", regex=True)\n",
    "    df['NameDesc'] = df['NameDesc'].str.replace(\"  +\", \" \", regex=True)\n",
    "    df['NameDesc'] = df['NameDesc'].str.replace(\"[^A-Za-z]\", \" \", regex=True)\n",
    "    return df\n",
    "\n",
    "def rm_stopwords(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['NameDesc'] = df['NameDesc'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    # print(f\"Removed stopwords:\\n {df.head(3).NameDesc}\\n\")\n",
    "    return df\n",
    "\n",
    "def lemmatize(df):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    def lemmatize_tokens(tokens):\n",
    "        def get_wordnet_pos(word):\n",
    "            tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "            tag_dict = {\"J\": wordnet.ADJ,\n",
    "                        \"N\": wordnet.NOUN,\n",
    "                        \"V\": wordnet.VERB,\n",
    "                        \"R\": wordnet.ADV}\n",
    "            return tag_dict.get(tag, wordnet.NOUN)\n",
    "        lemmas = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "        return lemmas\n",
    "    df['NameDesc'] = df['NameDesc'].apply(lambda x: lemmatize_tokens(x))\n",
    "    # print(f\"Lemmatized words:\\n {df.head(3).NameDesc}\")\n",
    "    return df\n",
    "\n",
    "def text_preprocessing(df):\n",
    "    basic_processing(df)\n",
    "    df['NameDesc'] = df['NameDesc'].apply(lambda x: word_tokenize(x))\n",
    "    rm_stopwords(df)\n",
    "    lemmatize(df)\n",
    "\n",
    "    k = random.randint(0, len(df)) # arbitary row to show that words have been removed\n",
    "    print(f\"Bef rm duplicates: {len(df.iloc[k]['NameDesc'])}\")\n",
    "    df['NameDesc'] = df['NameDesc'].apply(lambda x: list(set([word.lower() for word in x]))) # to remove duplicates\n",
    "    print(f\"Aft rm duplicates: {len(df.iloc[k]['NameDesc'])}\")\n",
    "    print(f\"Removed duplicates:\\n {df.head(3).NameDesc}\")\n",
    "\n",
    "    print(\"=========================================\")\n",
    "    return df\n",
    "\n",
    "def change_label(df):\n",
    "    stride_mapping = { # STRIDE\n",
    "        1: 1, # E\n",
    "        10: 2, # D\n",
    "        100: 3, # I\n",
    "        1000: 4, # R\n",
    "        10000: 5, # T\n",
    "        100000: 0, # S\n",
    "        11000: 4,\n",
    "        100100: 0\n",
    "    }\n",
    "    df['STRIDE'] = df['STRIDE'].map(stride_mapping)\n",
    "\n",
    "def split_data(df, train_set_size, test_set_size):\n",
    "    '''\n",
    "    train_set_size + test_set_size + dev_set_size = 1\n",
    "    '''\n",
    "    while True:\n",
    "        df_train, temp = train_test_split(df, test_size=1-train_set_size)\n",
    "        df_test, df_dev = train_test_split(temp, test_size=test_set_size)\n",
    "\n",
    "        change_label(df_train)\n",
    "        change_label(df_test)\n",
    "        change_label(df_dev)\n",
    "\n",
    "        c = set([0, 1, 2, 3, 4, 5])\n",
    "        if set(df_train['STRIDE'].unique()) != c or \\\n",
    "            set(df_test['STRIDE'].unique()) != c or \\\n",
    "            set(df_dev['STRIDE'].unique()) != c:\n",
    "                continue\n",
    "        else: break\n",
    "    return df_train, df_test, df_dev\n",
    "\n",
    "# keywords defined from each STRIDE category\n",
    "S = ['authenticate', 'username', 'password', 'access'] #add more words\n",
    "T = ['modify', 'persistent', 'database', 'alter', 'open', 'network', 'internet'] #add more words\n",
    "R = ['deny', 'action', 'prove', 'non-repudiation', 'item', 'sign', 'receipt', 'receive', 'evidence', 'package', 'untrace',]\n",
    "I = ['exposure', 'individual', 'access', 'file', 'granted', 'intruder', 'transit']\n",
    "D = ['denial', 'service', 'dos', 'web', 'server', 'unavailable', 'unusable', 'system', 'available', 'reliable']\n",
    "E = ['unprivileged', 'privileged', 'access', 'compromise', 'entire', 'system', 'elevation', 'penetrate', 'defenses', 'untrusted', 'trusted']\n",
    "\n",
    "# words to keep (manual filtering from keywords extracted from text_preprocessing())\n",
    "S_keep = ['information', 'detection', 'take',  'include', 'malicious', 'control', 'network', 'search', 'name', 'access', 'infrastructure', 'traffic', 'data', 'suspicious', 'trust', 'reconnaissance', 'email', 'phishing', 'resource', 'initial', 'visibility', 'monitor', 'server', 'form', 'open', 'potentially', 'websites', 'address', 'process', 'detect', 'credential', 'file', 'certificate', 'internet', 'install', 'key', 'online', 'link', 'source']\n",
    "E_keep = ['process', 'access', 'file', 'execute', 'activity', 'execution', 'network', 'behavior', 'create', 'control', 'log', 'privilege', 'application', 'service', 'within', 'event', 'account', 'modify', 'run', 'abuse', 'monitoring', 'environment', 'binary', 'credential', 'enable', 'api', 'exe', 'function', 'payload', 'target', 'method', 'services', 'launch', 'root', 'os', 'many''accounts']\n",
    "D_keep = ['service', 'target', 'tool', 'command', 'cause', 'server', 'network', 'outside', 'denial', 'dos', 'availability', 'high', 'destruction', 'infrastructure']\n",
    "I_keep = ['data', 'network', 'activity', 'access', 'behavior', 'environment', 'process', 'detection', 'remote', 'base', 'target', 'tool', 'file', 'api', 'traffic', 'acquire', 'application', 'host', 'infrastructure', 'device']\n",
    "R_keep = ['user', 'application', 'api', 'activity', 'audit', 'source', 'system', 'native', 'hide', 'error', 'intrusion', 'function', 'record', 'clear', 'gcp', 'permission', 'analysis', 'collection', 'updatesink', 'indicate', 'detection', 'data', 'collect', 'environment', 'call', 'limit', 'cloudtrail', 'loss', 'conduct', 'prior', 'delete', 'cloud', 'configservicev', 'cloudwatch', 'diagnostic', 'capability', 'sufficient', 'insight', 'avoid']\n",
    "T_keep = ['malicious', 'file', 'activity', 'process', 'execute', 'access', 'information', 'control', 'software', 'modify', 'network', 'data', 'abuse', 'exe', 'manipulate', 'bypass', 'malware', 'functionality', 'integrity', 'dll', 'anomaly', 'install']\n",
    "\n",
    "# combine both lists together by group\n",
    "S_final = S + S_keep\n",
    "T_final = T + T_keep\n",
    "R_final = R + R_keep\n",
    "I_final = I + I_keep\n",
    "D_final = D + D_keep\n",
    "E_final = E + E_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "base_classifier = LogisticRegression()\n",
    "\n",
    "clf = OneVsRestClassifier(base_classifier)\n",
    "\n",
    "def vectorize(df_train, df_test):\n",
    "    df_train['NameDesc'] = df_train['NameDesc'].apply(lambda x: ' '.join(x))\n",
    "    df_test['NameDesc'] = df_test['NameDesc'].apply(lambda x: ' '.join(x))\n",
    "    # df_dev['NameDesc'] = df_dev['NameDesc'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(df_train['NameDesc']).toarray()\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(df_test['NameDesc']).toarray()\n",
    "    # X_val_tfidf = tfidf_vectorizer.transform(df_dev['NameDesc']).toarray()\n",
    "\n",
    "    y_train = df_train['STRIDE'].values\n",
    "    y_test = df_test['STRIDE'].values\n",
    "    # y_val = df_dev['STRIDE'].values\n",
    "\n",
    "    return X_train_tfidf, X_test_tfidf, y_train, y_test\n",
    "\n",
    "def train_loop(X_train_tfidf, X_test_tdift, y_train, y_test):\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_train_tfidf)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return y_pred, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data2(df, train_set_size):\n",
    "    '''\n",
    "    train_set_size + test_set_size + dev_set_size = 1\n",
    "    '''\n",
    "    while True:\n",
    "        df_train, df_test = train_test_split(df, test_size=1 - train_set_size)\n",
    "        \n",
    "        # Ensure both df_train and df_test have the same number of samples\n",
    "        # min_samples = min(len(df_train), len(df_test))\n",
    "        # df_train = df_train.sample(n=min_samples)\n",
    "        # df_test = df_test.sample(n=min_samples)\n",
    "\n",
    "        change_label(df_train)\n",
    "        change_label(df_test)\n",
    "\n",
    "        c = set([0, 1, 2, 3, 4, 5])\n",
    "        if set(df_train['STRIDE'].unique()) != c or set(df_test['STRIDE'].unique()) != c:\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split:\n",
      "df_train:\n",
      "STRIDE\n",
      "1    62\n",
      "3    40\n",
      "5    28\n",
      "0    13\n",
      "4     1\n",
      "2     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "df_test:\n",
      "STRIDE\n",
      "1    154\n",
      "3    113\n",
      "5     38\n",
      "0     21\n",
      "2     10\n",
      "4      4\n",
      "Name: count, dtype: int64\n",
      "=========================================\n",
      "\n",
      "Bef rm duplicates: 46\n",
      "Aft rm duplicates: 36\n",
      "Removed duplicates:\n",
      " 0    [associate, media, rate, organization, reconna...\n",
      "1    [associate, protect, wfp, protocol, advantage,...\n",
      "2    [network, success, across, protocol, event, co...\n",
      "Name: NameDesc, dtype: object\n",
      "=========================================\n",
      "Bef rm duplicates: 154\n",
      "Aft rm duplicates: 103\n",
      "Removed duplicates:\n",
      " 0    [exist, associate, web, organization, control,...\n",
      "1    [masked, network, sequence, order, commonly, r...\n",
      "2    [outdated, control, network, support, interpre...\n",
      "Name: NameDesc, dtype: object\n",
      "=========================================\n",
      "Trivial text preprocessing:\n",
      "df_train:\n",
      "                                            NameDesc  STRIDE\n",
      "0  [associate, media, rate, organization, reconna...       0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = '../data/external/mitre-classified.xlsx'\n",
    "df = pd.read_excel(PATH)\n",
    "\n",
    "# train test dev split\n",
    "df_train, df_test = split_data2(df, train_set_size=0.3)\n",
    "\n",
    "col_toDrop = ['Ref', 'Name', 'Desc', 'Confidentiality', 'Integrity', 'Availability', 'Ease Of Exploitation', 'References', 'Unnamed: 0']\n",
    "df_train = df_train.reset_index(drop=True).drop(columns=col_toDrop)\n",
    "df_test = df_test.reset_index(drop=True).drop(columns=col_toDrop)\n",
    "# df_dev = df_dev.reset_index(drop=True).drop(columns=col_toDrop)\n",
    "print(\"Data split:\")\n",
    "print(f\"df_train:\\n{df_train['STRIDE'].value_counts()}\\n\")\n",
    "# print(f\"df_dev:\\n{df_dev['STRIDE'].value_counts()}\\n\")\n",
    "print(f\"df_test:\\n{df_test['STRIDE'].value_counts()}\")\n",
    "print(\"=========================================\\n\")\n",
    "\n",
    "\n",
    "# trivially extract keywords\n",
    "df_train = text_preprocessing(df_train)\n",
    "df_test = text_preprocessing(df_test)\n",
    "# df_dev = text_preprocessing(df_dev)\n",
    "print(\"Trivial text preprocessing:\")\n",
    "print(f\"df_train:\\n{df_train.head(1)}\\n\")\n",
    "\n",
    "# obtain better set of keywords\n",
    "df_train = better_keywords(df_train)\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = vectorize(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes after vectorization:\n",
      "X_train_tfidf shape: (145, 1324)\n",
      "X_test_tfidf shape: (340, 1324)\n",
      "y_train shape: (145,)\n",
      "y_test shape: (340,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes after vectorization:\")\n",
    "print(\"X_train_tfidf shape:\", X_train_tfidf.shape)\n",
    "print(\"X_test_tfidf shape:\", X_test_tfidf.shape)\n",
    "# print(\"X_val_tfidf shape:\", X_val_tfidf.shape)    \n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "# print(\"y_val shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5588235294117647\n",
      "Precision: 0.4857335640138408\n",
      "Recall: 0.5588235294117647\n",
      "F1-score: 0.46375097449967534\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0  15   0   5   0   1]\n",
      " [  0 153   0   1   0   0]\n",
      " [  0   5   0   5   0   0]\n",
      " [  0  76   0  37   0   0]\n",
      " [  0   4   0   0   0   0]\n",
      " [  0  36   0   2   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chufeng/Documents/IARCS internship/code/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Results:\n",
      "Accuracy: 0.5470588235294118\n",
      "Precision: 0.6052904539481165\n",
      "Recall: 0.5470588235294118\n",
      "F1-score: 0.4652576015371067\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  5  16   0   0   0   0]\n",
      " [  0 151   0   2   0   1]\n",
      " [  0   8   0   2   0   0]\n",
      " [ 13  72   0  28   0   0]\n",
      " [  0   4   0   0   0   0]\n",
      " [  1  33   0   2   0   2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chufeng/Documents/IARCS internship/code/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_rf = RandomForestClassifier()\n",
    "clf_rf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_rf = clf_rf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
    "recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest Classifier Results:\")\n",
    "print(\"Accuracy:\", accuracy_rf)\n",
    "print(\"Precision:\", precision_rf)\n",
    "print(\"Recall:\", recall_rf)\n",
    "print(\"F1-score:\", f1_rf)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, acc = train_loop(X_train_tfidf, X_test_tfidf, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
