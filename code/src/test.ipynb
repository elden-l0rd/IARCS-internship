{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as rand\n",
    "\n",
    "OPATH = '../data/external/mitre-classified.xlsx'\n",
    "PATH = '../data/results/translated_bn.xlsx'\n",
    "df0 = pd.read_excel(OPATH)\n",
    "df = pd.read_excel(PATH)\n",
    "\n",
    "row = df.loc[rand.randint(0, len(df)), ['Ref', 'NameDesc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T-T1574.013'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.Ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KernelCallbackTable Adversaries may abuse the <code>KernelCallbackTable</code> of a process to hijack its execution flow in order to run their own payloads.(Citation: Lazarus APT January 2022)(Citation: FinFisher exposed ) The <code>KernelCallbackTable</code> can be found in the Process Environment Block (PEB) and is initialized to an array of graphic functions available to a GUI process once <code>user32.dll</code> is loaded.(Citation: Windows Process Injection KernelCallbackTable)<br><br>An adversary may hijack the execution flow of a process using the <code>KernelCallbackTable</code> by replacing an original callback function with a malicious payload. Modifying callback functions can be achieved in various ways involving related behaviors such as [Reflective Code Loading](https://attack.mitre.org/techniques/T1620) or [Process Injection](https://attack.mitre.org/techniques/T1055) into another process.<br><br>A pointer to the memory address of the <code>KernelCallbackTable</code> can be obtained by locating the PEB (ex: via a call to the <code>NtQueryInformationProcess()</code> [Native API](https://attack.mitre.org/techniques/T1106) function).(Citation: NtQueryInformationProcess) Once the pointer is located, the <code>KernelCallbackTable</code> can be duplicated, and a function in the table (e.g., <code>fnCOPYDATA</code>) set to the address of a malicious payload (ex: via <code>WriteProcessMemory()</code>). The PEB is then updated with the new address of the table. Once the tampered function is invoked, the malicious payload will be triggered.(Citation: Lazarus APT January 2022)<br><br>The tampered function is typically invoked using a Windows message. After the process is hijacked and malicious code is executed, the <code>KernelCallbackTable</code> may also be restored to its original state by the rest of the malicious payload.(Citation: Lazarus APT January 2022) Use of the <code>KernelCallbackTable</code> to hijack execution flow may evade detection from security products since the execution can be masked under a legitimate process.<br><br>Analyze process behavior to determine if a process is performing actions it usually does not, such as opening network connections, reading files, or other suspicious behaviors that could relate to post-compromise behavior.<br><br>Monitoring Windows API calls indicative of the various types of code injection may generate a significant amount of data and may not be directly useful for defense unless collected under specific circumstances. for known bad sequence of calls, since benign use of API functions may be common and difficult to distinguish from malicious behavior. Windows API calls such as <code>WriteProcessMemory()</code> and <code>NtQueryInformationProcess()</code> with the parameter set to <code>ProcessBasicInformation</code> may be used for this technique.(Citation: Lazarus APT January 2022)\n"
     ]
    }
   ],
   "source": [
    "matches = df0[df0['Ref'] == row.Ref]\n",
    "\n",
    "# Check if there are any matches\n",
    "if not matches.empty:\n",
    "    for _, row in matches.iterrows():\n",
    "        print(f\"{row['NameDesc']}\")\n",
    "else:\n",
    "    print(f\"No matches found for Ref {row.Ref}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KernelCallbackTable Adversaries may abuse the <code>KernelCallbackTable</code> of a process to hijack its execution flow in order to run their own payloads.(Citation: Lazarus APT January 2022)(Citation: FinFisher exposed ) The <code>KernelCallbackTable</code> can be found in the Process Environment Block (PEB) and is initialized to an array of graphic functions available to a GUI process once <code>user32.dll</code> is loaded.(Citation: Windows Process Injection KernelCallbackTable)<br><br>An adversary may hijack the execution flow of a process using the <code>KernelCallbackTable</code> by replacing an original callback function with a malicious payload. Modifying callback functions can be achieved in various ways involving related behaviors such as [Reflective Code Loading](https://attack.mitre.org/techniques/T1620) or [Process Injection](https://attack.mitre.org/techniques/T1055) into another process.<br><br>A pointer to the memory address of the <code>KernelCallbackTable</code> can be obtained by locating the PEB (ex: via a call to the <code>NtQueryInformationProcess()</code> [Native API](https://attack.mitre.org/techniques/T1106) function).(Citation: NtQueryInformationProcess) Once the pointer is located, the <code>KernelCallbackTable</code> can be duplicated, and a function in the table (e.g., <code>fnCOPYDATA</code>) set to the address of a malicious payload (ex: via <code>WriteProcessMemory()</code>). The PEB is then updated with the new address of the table. Once the tampered function is invoked, the malicious payload will be triggered.(Citation: Lazarus APT January 2022)<br><br>The tampered function is typically invoked using a Windows message. After the process is hijacked and malicious code is executed, the <code>KernelCallbackTable</code> may also be restored to its original state by the rest of the malicious payload.(Citation: Lazarus APT January 2022) Use of the <code>KernelCallbackTable</code> to hijack execution flow may evade detection from security products since the execution can be masked under a legitimate process.<br><br>Analyze process behavior to determine if a process is performing actions it usually does not, such as opening network connections, reading files, or other suspicious behaviors that could relate to post-compromise behavior.<br><br>Monitoring Windows API calls indicative of the various types of code injection may generate a significant amount of data and may not be directly useful for defense unless collected under specific circumstances. for known bad sequence of calls, since benign use of API functions may be common and difficult to distinguish from malicious behavior. Windows API calls such as <code>WriteProcessMemory()</code> and <code>NtQueryInformationProcess()</code> with the parameter set to <code>ProcessBasicInformation</code> may be used for this technique.(Citation: Lazarus APT January 2022)\n"
     ]
    }
   ],
   "source": [
    "print(row.NameDesc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y3/qz4wgjts6dj2m3kdg2nq67200000gn/T/ipykernel_46967/1592899246.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chufeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/chufeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/chufeng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/chufeng/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Loading Word2Vec Model: 100%|██████████| 1/1 [00:18<00:00, 18.69s/model]\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chufeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/chufeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/chufeng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/chufeng/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Loading Word2Vec Model: 100%|██████████| 1/1 [00:18<00:00, 18.34s/model]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 15:21:51.707 Python[46967:2446942] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from modules import preprocessing as pre\n",
    "from modules import extract_keywords as ek\n",
    "from modules import visualise as vis\n",
    "import models.ovr_model as ovr\n",
    "import hyperparm as hpt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim.downloader\n",
    "from tqdm import tqdm\n",
    "\n",
    "MIN_COSINE_VALUE = 0.28\n",
    "\n",
    "def load_word2vec_model():\n",
    "    with tqdm(total=1, desc=\"Loading Word2Vec Model\", unit=\"model\") as pbar:\n",
    "        w2v_model = gensim.downloader.load('word2vec-google-news-300')\n",
    "        pbar.update(1)\n",
    "    return w2v_model\n",
    "\n",
    "w2v = load_word2vec_model()\n",
    "\n",
    "def merge_lists(l1, min_cosine_value, w2v, ref_list):\n",
    "    final_list = []\n",
    "    \n",
    "    for word in l1:\n",
    "        if word in w2v:\n",
    "            similarities = []\n",
    "            for ref_word in ref_list:\n",
    "                if ref_word in w2v:\n",
    "                    sim = w2v.similarity(word, ref_word)\n",
    "                    similarities.append(sim)\n",
    "            if any(sim >= min_cosine_value for sim in similarities):\n",
    "                final_list.append(word)\n",
    "    if 'use' in final_list: final_list.remove('use')\n",
    "    elif 'also' in final_list: final_list.remove('also')\n",
    "    return final_list\n",
    "\n",
    "def better_keywords(df):\n",
    "    for i in range(len(df)):\n",
    "        if df.iloc[i]['STRIDE'] == 0:\n",
    "            updated_list = merge_lists(df.loc[i, 'NameDesc'], MIN_COSINE_VALUE, w2v, S_final)\n",
    "        elif df.iloc[i]['STRIDE'] == 1:\n",
    "            updated_list = merge_lists(df.loc[i, 'NameDesc'], MIN_COSINE_VALUE, w2v, E_final)\n",
    "        elif df.iloc[i]['STRIDE'] == 2:\n",
    "            updated_list = merge_lists(df.loc[i, 'NameDesc'], MIN_COSINE_VALUE, w2v, D_final)\n",
    "        elif df.iloc[i]['STRIDE'] == 3:\n",
    "            updated_list = merge_lists(df.loc[i, 'NameDesc'], MIN_COSINE_VALUE, w2v, I_final)\n",
    "        elif df.iloc[i]['STRIDE'] == 4:\n",
    "            updated_list = merge_lists(df.loc[i, 'NameDesc'], MIN_COSINE_VALUE, w2v, R_final)\n",
    "        else:\n",
    "            updated_list = merge_lists(df.loc[i, 'NameDesc'], MIN_COSINE_VALUE, w2v, T_final)\n",
    "        \n",
    "        df.at[i, 'NameDesc'] = updated_list\n",
    "    return df\n",
    "\n",
    "def basic_processing(df):\n",
    "    words_to_remove = [\"e.g.\", \"code\", \"may\", \"attack\", \"system\", \"adversary\", \"Adversaries\"]\n",
    "    for word in words_to_remove:\n",
    "        df['NameDesc'] = df['NameDesc'].apply(lambda x: x.replace(word, ''))\n",
    "    for word in words_to_remove:\n",
    "        df['NameDesc'] = df['NameDesc'].apply(lambda x: re.sub(r'\\b' + re.escape(word) + r'\\b', '', x))\n",
    "\n",
    "    # df['NameDesc'] = df['NameDesc'].str.replace(r\"\\b(\" + \"|\".join(words_to_remove) + r\")\\b\", \"\", regex=True)\n",
    "    df['NameDesc'] = df['NameDesc'].str.replace(\"<br><br>\", \"\", regex=True)\n",
    "    df['NameDesc'] = df['NameDesc'].str.replace(\"\\(Citation:.*?\\)\", \"\", regex=True)\n",
    "    df['NameDesc'] = df['NameDesc'].str.replace(\"http\\S+\", \"\", regex=True)\n",
    "    df['NameDesc'] = df['NameDesc'].str.replace(\"  +\", \" \", regex=True)\n",
    "    df['NameDesc'] = df['NameDesc'].str.replace(\"[^A-Za-z]\", \" \", regex=True)\n",
    "    return df\n",
    "\n",
    "def rm_stopwords(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['NameDesc'] = df['NameDesc'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    # print(f\"Removed stopwords:\\n {df.head(3).NameDesc}\\n\")\n",
    "    return df\n",
    "\n",
    "def lemmatize(df):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    def lemmatize_tokens(tokens):\n",
    "        def get_wordnet_pos(word):\n",
    "            tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "            tag_dict = {\"J\": wordnet.ADJ,\n",
    "                        \"N\": wordnet.NOUN,\n",
    "                        \"V\": wordnet.VERB,\n",
    "                        \"R\": wordnet.ADV}\n",
    "            return tag_dict.get(tag, wordnet.NOUN)\n",
    "        lemmas = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "        return lemmas\n",
    "    df['NameDesc'] = df['NameDesc'].apply(lambda x: lemmatize_tokens(x))\n",
    "    # print(f\"Lemmatized words:\\n {df.head(3).NameDesc}\")\n",
    "    return df\n",
    "\n",
    "def text_preprocessing(df):\n",
    "    basic_processing(df)\n",
    "    df['NameDesc'] = df['NameDesc'].apply(lambda x: word_tokenize(x))\n",
    "    rm_stopwords(df)\n",
    "    lemmatize(df)\n",
    "\n",
    "    k = random.randint(0, len(df)) # arbitary row to show that words have been removed\n",
    "    print(f\"Bef rm duplicates: {len(df.iloc[k]['NameDesc'])}\")\n",
    "    df['NameDesc'] = df['NameDesc'].apply(lambda x: list(set([word.lower() for word in x]))) # to remove duplicates\n",
    "    print(f\"Aft rm duplicates: {len(df.iloc[k]['NameDesc'])}\")\n",
    "    print(f\"Removed duplicates:\\n {df.head(3).NameDesc}\")\n",
    "\n",
    "    print(\"=========================================\")\n",
    "    return df\n",
    "\n",
    "def change_label(df):\n",
    "    stride_mapping = { # STRIDE\n",
    "        1: 1, # E\n",
    "        10: 2, # D\n",
    "        100: 3, # I\n",
    "        1000: 4, # R\n",
    "        10000: 5, # T\n",
    "        100000: 0, # S\n",
    "        11000: 4,\n",
    "        100100: 0\n",
    "    }\n",
    "    df['STRIDE'] = df['STRIDE'].map(stride_mapping)\n",
    "\n",
    "def split_data(df, train_set_size, test_set_size):\n",
    "    '''\n",
    "    train_set_size + test_set_size + dev_set_size = 1\n",
    "    '''\n",
    "    while True:\n",
    "        df_train, temp = train_test_split(df, test_size=1-train_set_size)\n",
    "        df_test, df_dev = train_test_split(temp, test_size=test_set_size)\n",
    "\n",
    "        change_label(df_train)\n",
    "        change_label(df_test)\n",
    "        change_label(df_dev)\n",
    "\n",
    "        c = set([0, 1, 2, 3, 4, 5])\n",
    "        if set(df_train['STRIDE'].unique()) != c or \\\n",
    "            set(df_test['STRIDE'].unique()) != c or \\\n",
    "            set(df_dev['STRIDE'].unique()) != c:\n",
    "                continue\n",
    "        else: break\n",
    "    return df_train, df_test, df_dev\n",
    "\n",
    "# keywords defined from each STRIDE category\n",
    "S = ['authenticate', 'username', 'password', 'access'] #add more words\n",
    "T = ['modify', 'persistent', 'database', 'alter', 'open', 'network', 'internet'] #add more words\n",
    "R = ['deny', 'action', 'prove', 'non-repudiation', 'item', 'sign', 'receipt', 'receive', 'evidence', 'package', 'untrace',]\n",
    "I = ['exposure', 'individual', 'access', 'file', 'granted', 'intruder', 'transit']\n",
    "D = ['denial', 'service', 'dos', 'web', 'server', 'unavailable', 'unusable', 'system', 'available', 'reliable']\n",
    "E = ['unprivileged', 'privileged', 'access', 'compromise', 'entire', 'system', 'elevation', 'penetrate', 'defenses', 'untrusted', 'trusted']\n",
    "\n",
    "# words to keep (manual filtering from keywords extracted from text_preprocessing())\n",
    "S_keep = ['information', 'detection', 'take',  'include', 'malicious', 'control', 'network', 'search', 'name', 'access', 'infrastructure', 'traffic', 'data', 'suspicious', 'trust', 'reconnaissance', 'email', 'phishing', 'resource', 'initial', 'visibility', 'monitor', 'server', 'form', 'open', 'potentially', 'websites', 'address', 'process', 'detect', 'credential', 'file', 'certificate', 'internet', 'install', 'key', 'online', 'link', 'source']\n",
    "E_keep = ['process', 'access', 'file', 'execute', 'activity', 'execution', 'network', 'behavior', 'create', 'control', 'log', 'privilege', 'application', 'service', 'within', 'event', 'account', 'modify', 'run', 'abuse', 'monitoring', 'environment', 'binary', 'credential', 'enable', 'api', 'exe', 'function', 'payload', 'target', 'method', 'services', 'launch', 'root', 'os', 'many''accounts']\n",
    "D_keep = ['service', 'target', 'tool', 'command', 'cause', 'server', 'network', 'outside', 'denial', 'dos', 'availability', 'high', 'destruction', 'infrastructure']\n",
    "I_keep = ['data', 'network', 'activity', 'access', 'behavior', 'environment', 'process', 'detection', 'remote', 'base', 'target', 'tool', 'file', 'api', 'traffic', 'acquire', 'application', 'host', 'infrastructure', 'device']\n",
    "R_keep = ['user', 'application', 'api', 'activity', 'audit', 'source', 'system', 'native', 'hide', 'error', 'intrusion', 'function', 'record', 'clear', 'gcp', 'permission', 'analysis', 'collection', 'updatesink', 'indicate', 'detection', 'data', 'collect', 'environment', 'call', 'limit', 'cloudtrail', 'loss', 'conduct', 'prior', 'delete', 'cloud', 'configservicev', 'cloudwatch', 'diagnostic', 'capability', 'sufficient', 'insight', 'avoid']\n",
    "T_keep = ['malicious', 'file', 'activity', 'process', 'execute', 'access', 'information', 'control', 'software', 'modify', 'network', 'data', 'abuse', 'exe', 'manipulate', 'bypass', 'malware', 'functionality', 'integrity', 'dll', 'anomaly', 'install']\n",
    "\n",
    "# combine both lists together by group\n",
    "S_final = S + S_keep\n",
    "T_final = T + T_keep\n",
    "R_final = R + R_keep\n",
    "I_final = I + I_keep\n",
    "D_final = D + D_keep\n",
    "E_final = E + E_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "base_classifier = LogisticRegression()\n",
    "\n",
    "clf = OneVsRestClassifier(base_classifier)\n",
    "\n",
    "def vectorize(df_train, df_test):\n",
    "    df_train['NameDesc'] = df_train['NameDesc'].apply(lambda x: ' '.join(x))\n",
    "    df_test['NameDesc'] = df_test['NameDesc'].apply(lambda x: ' '.join(x))\n",
    "    # df_dev['NameDesc'] = df_dev['NameDesc'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(df_train['NameDesc']).toarray()\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(df_test['NameDesc']).toarray()\n",
    "    # X_val_tfidf = tfidf_vectorizer.transform(df_dev['NameDesc']).toarray()\n",
    "\n",
    "    y_train = df_train['STRIDE'].values\n",
    "    y_test = df_test['STRIDE'].values\n",
    "    # y_val = df_dev['STRIDE'].values\n",
    "\n",
    "    return X_train_tfidf, X_test_tfidf, y_train, y_test\n",
    "\n",
    "def train_loop(X_train_tfidf, X_test_tdift, y_train, y_test):\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_train_tfidf)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return y_pred, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data2(df, train_set_size):\n",
    "    '''\n",
    "    train_set_size + test_set_size + dev_set_size = 1\n",
    "    '''\n",
    "    while True:\n",
    "        df_train, df_test = train_test_split(df, test_size=1 - train_set_size)\n",
    "        \n",
    "        # Ensure both df_train and df_test have the same number of samples\n",
    "        min_samples = min(len(df_train), len(df_test))\n",
    "        df_train = df_train.sample(n=min_samples)\n",
    "        df_test = df_test.sample(n=min_samples)\n",
    "\n",
    "        change_label(df_train)\n",
    "        change_label(df_test)\n",
    "\n",
    "        c = set([0, 1, 2, 3, 4, 5])\n",
    "        if set(df_train['STRIDE'].unique()) != c or set(df_test['STRIDE'].unique()) != c:\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split:\n",
      "df_train:\n",
      "STRIDE\n",
      "1    67\n",
      "3    41\n",
      "5    26\n",
      "0     6\n",
      "4     3\n",
      "2     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "df_test:\n",
      "STRIDE\n",
      "1    64\n",
      "3    46\n",
      "5    18\n",
      "0    10\n",
      "2     5\n",
      "4     2\n",
      "Name: count, dtype: int64\n",
      "=========================================\n",
      "\n",
      "Bef rm duplicates: 136\n",
      "Aft rm duplicates: 83\n",
      "Removed duplicates:\n",
      " 0    [operation, person, target, take, web, uploade...\n",
      "1    [monitor, specify, trigger, handle, allows, ct...\n",
      "2    [monitor, include, via, manage, secret, call, ...\n",
      "Name: NameDesc, dtype: object\n",
      "=========================================\n",
      "Bef rm duplicates: 221\n",
      "Aft rm duplicates: 136\n",
      "Removed duplicates:\n",
      " 0    [via, operation, custom, data, detect, impleme...\n",
      "1    [make, x, require, powershell, within, privile...\n",
      "2    [try, search, via, call, access, design, typic...\n",
      "Name: NameDesc, dtype: object\n",
      "=========================================\n",
      "Trivial text preprocessing:\n",
      "df_train:\n",
      "                                            NameDesc  STRIDE\n",
      "0  [operation, person, target, take, web, uploade...       0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = '../data/external/mitre-classified.xlsx'\n",
    "df = pd.read_excel(PATH)\n",
    "\n",
    "# train test dev split\n",
    "df_train, df_test = split_data2(df, train_set_size=0.3)\n",
    "\n",
    "col_toDrop = ['Ref', 'Name', 'Desc', 'Confidentiality', 'Integrity', 'Availability', 'Ease Of Exploitation', 'References', 'Unnamed: 0']\n",
    "df_train = df_train.reset_index(drop=True).drop(columns=col_toDrop)\n",
    "df_test = df_test.reset_index(drop=True).drop(columns=col_toDrop)\n",
    "# df_dev = df_dev.reset_index(drop=True).drop(columns=col_toDrop)\n",
    "print(\"Data split:\")\n",
    "print(f\"df_train:\\n{df_train['STRIDE'].value_counts()}\\n\")\n",
    "# print(f\"df_dev:\\n{df_dev['STRIDE'].value_counts()}\\n\")\n",
    "print(f\"df_test:\\n{df_test['STRIDE'].value_counts()}\")\n",
    "print(\"=========================================\\n\")\n",
    "\n",
    "\n",
    "# trivially extract keywords\n",
    "df_train = text_preprocessing(df_train)\n",
    "df_test = text_preprocessing(df_test)\n",
    "# df_dev = text_preprocessing(df_dev)\n",
    "print(\"Trivial text preprocessing:\")\n",
    "print(f\"df_train:\\n{df_train.head(1)}\\n\")\n",
    "\n",
    "# obtain better set of keywords\n",
    "df_train = better_keywords(df_train)\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = vectorize(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Application Access Token Adversaries may use stolen application access tokens to bypass the typical authentication process and access restricted accounts, information, or services on remote systems. These tokens are typically stolen from users or services and used in lieu of login credentials.<br><br>Application access tokens are used to make authorized API requests on behalf of a user or service and are commonly used as a way to access resources in cloud and container-based applications and software-as-a-service (SaaS).(Citation: Auth0 - Why You Should Always Use Access Tokens to Secure APIs Sept 2019) <br><br>In AWS and GCP environments, adversaries can trigger a request for a short-lived access token with the privileges of another user account.(Citation: Google Cloud Service Account Credentials)(Citation: AWS Temporary Security Credentials) The adversary can then use this token to request data or perform actions the original account could not. If permissions for this feature are misconfigured - for example, by allowing all users to request a token for a particular account - an adversary may be able to gain initial access to a Cloud Account or escalate their privileges.(Citation: Rhino Security Labs Enumerating AWS Roles)<br><br>OAuth is one commonly implemented framework that issues tokens to users for access to systems. These frameworks are used collaboratively to verify the user and determine what actions the user is allowed to perform. Once identity is established, the token allows actions to be authorized, without passing the actual credentials of the user. Therefore, compromise of the token can grant the adversary access to resources of other sites through a malicious application.(Citation: okta)<br><br>For example, with a cloud-based email service once an OAuth access token is granted to a malicious application, it can potentially gain long-term access to features of the user account if a \"refresh\" token enabling background access is awarded.(Citation: Microsoft Identity Platform Access 2019) With an OAuth access token an adversary can use the user-granted REST API to perform functions such as email searching and contact enumeration.(Citation: Staaldraad Phishing with OAuth 2017)<br><br>Compromised access tokens may be used as an initial step in compromising other services. For example, if a token grants access to a victim's primary email, the adversary may be able to extend access to all other services which the target subscribes by triggering forgotten password routines. Direct API access through a token negates the effectiveness of a second authentication factor and may be immune to intuitive countermeasures like changing passwords. Access abuse over an API channel can be difficult to detect even from the service provider end, as the access can still align well with a legitimate workflow.<br><br>Monitor access token activity for abnormal use and permissions granted to unusual or suspicious applications and APIs. Additionally, administrators should review logs for calls to the AWS Security Token Service (STS) and usage of GCP service accounts in order to identify anomalous actions.(Citation: AWS Logging IAM Calls)(Citation: GCP Monitoring Service Account Usage)\n",
      "1\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(df.iloc[87]['NameDesc'])\n",
    "print(df.iloc[87]['STRIDE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shapes after vectorization:\")\n",
    "print(\"X_train_tfidf shape:\", X_train_tfidf.shape)\n",
    "print(\"X_test_tfidf shape:\", X_test_tfidf.shape)\n",
    "# print(\"X_val_tfidf shape:\", X_val_tfidf.shape)    \n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "# print(\"y_val shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_rf = RandomForestClassifier()\n",
    "clf_rf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_rf = clf_rf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
    "recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest Classifier Results:\")\n",
    "print(\"Accuracy:\", accuracy_rf)\n",
    "print(\"Precision:\", precision_rf)\n",
    "print(\"Recall:\", recall_rf)\n",
    "print(\"F1-score:\", f1_rf)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, acc = train_loop(X_train_tfidf, X_test_tfidf, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, Permute, Reshape, Multiply, Lambda, Softmax, Dot\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class SelfAttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', \n",
    "                                 shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias',\n",
    "                                 shape=(input_shape[-1],),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        super(SelfAttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # eij = tanh(Wx + b)\n",
    "        eij = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        \n",
    "        # ai = exp(eij) / sum_j(exp(eij))\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai / K.sum(ai, axis=2, keepdims=True)  # sum over the features dimension\n",
    "        \n",
    "        # weighted input\n",
    "        weighted_input = x * weights\n",
    "        return weighted_input\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Bidirectional, LSTM, Dense, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "vocab_size = X_train_tfidf.shape[1]\n",
    "embedding_dim = 128\n",
    "sequence_length = 500  # length of your input sequences\n",
    "num_classes = 6\n",
    "num_filters = 256\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "# Input Layer\n",
    "inputs = Input(shape=(vocab_size,))\n",
    "\n",
    "# Word Embedding Layer\n",
    "x = Embedding(vocab_size, embedding_dim)(inputs)\n",
    "\n",
    "# tanh activation layer\n",
    "x = Activation('tanh')(x)\n",
    "\n",
    "# Self-attention layer\n",
    "attention_output = SelfAttentionLayer()(x)\n",
    "x = Concatenate()([attention_output])\n",
    "\n",
    "# Convolutional Layers with different kernel sizes\n",
    "conv_blocks = []\n",
    "for i in range(3):\n",
    "    conv = Conv1D(filters=num_filters, kernel_size=kernel_size, padding=\"valid\", activation=\"relu\", strides=1)(x)\n",
    "    conv = GlobalMaxPooling1D()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "# concatenate results\n",
    "x = Concatenate()(conv_blocks)\n",
    "\n",
    "# BLSTM Layer\n",
    "blstm = Bidirectional(LSTM(128, return_sequences=True))(attention_output)\n",
    "x = Concatenate()([attention_output, blstm])\n",
    "blstm = GlobalMaxPooling1D()(x)\n",
    "\n",
    "# concatenate convolutional features and BLSTM features\n",
    "x = Concatenate()([x for x in conv_blocks])\n",
    "x = Concatenate()([x, blstm])\n",
    "\n",
    "# Fully Connected Layer\n",
    "x = Dense(128)(x)\n",
    "x = Activation('tanh')(x)\n",
    "\n",
    "# Output Layer\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Build and compile the model\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model, show_shapes=False, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "def train_loop(model, X_train_tfidf, y_train, X_val_tfidf, y_val, NUM_EPOCHS, BATCH_SIZE):\n",
    "    hist = model.fit(\n",
    "        X_train_tfidf, y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        validation_data=(X_val_tfidf, y_val),\n",
    "        verbose=1,\n",
    "        callbacks=[early_stop,]\n",
    "    )\n",
    "    return hist, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert labels to one-hot encoded format\n",
    "y_train_encoded = to_categorical(y_train, num_classes)\n",
    "y_test_encoded = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "y_pred, acc = train_loop(model, X_train_tfidf, y_train_encoded, X_test_tfidf, y_test_encoded, 100, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_data(hist, model, X_val_padded, y_val, classes):\n",
    "    # output_dir = 'data/results'\n",
    "\n",
    "    acc = hist.history['accuracy']\n",
    "    val_acc = hist.history['val_accuracy']\n",
    "    loss = hist.history['loss']\n",
    "    val_loss = hist.history['val_loss']\n",
    "    epochs_range = range(len(acc))\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_val_padded), axis=1)\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=classes, yticklabels=classes, square=True)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f\"{output_dir}/confusionmatrix.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.pause(2)\n",
    "    return\n",
    "\n",
    "def plot_cm(cm, model=0, X_val_padded=0, y_val=0, classes=0):\n",
    "    # output_dir = 'data/results'\n",
    "    if (model or X_val_padded or y_val or classes):\n",
    "        y_pred = np.argmax(model.predict(X_val_padded), axis=1)\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=classes, yticklabels=classes, square=True)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f\"{output_dir}/confusionmatrix.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.pause(2)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(hist=y_pred, model=acc, X_val_padded=X_test_tfidf, y_val=y_test, classes=[0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "!export REPLICATE_API_TOKEN=r8_cQ7dTejgV4S3VMwyYu5wiv7VUSlAT8I0ptqOH\n",
    "os.environ['REPLICATE_API_TOKEN'] = 'r8_cQ7dTejgV4S3VMwyYu5wiv7VUSlAT8I0ptqOH'\n",
    "import replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The STRIDE security framework is defined by Spoofing, Tampering, Repudiation, Information disclosure, Denial of service, Elevation of privilege. Using this, which category do you think the following attack should be classfied under? The attacker is able to access the system and modify the data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The meta/llama-2-13b-chat model can stream output as it's running.\n",
    "api = replicate.Client(api_token=os.environ[\"REPLICATE_API_TOKEN\"])\n",
    "output = api.run(\n",
    "    # \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "    \"meta/llama-2-13b-chat\",\n",
    "        input={\"prompt\": prompt,}\n",
    "    )\n",
    "for item in output:\n",
    "    print(item, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
