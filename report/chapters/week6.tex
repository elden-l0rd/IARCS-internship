\section*{Week 6: Setting up Llama 2}
\addcontentsline{toc}{section}{Week 6: Setting up Llama 2}

There was a lot of attempt in trying to setup new technologies that I was unfamiliar with this week. The summary is as follows:
\begin{itemize}[topsep=0pt]
    \item \textbf{6.1} Setup Llama 2 7B-chat model on my personal machine
    \item \textbf{6.2} Tried using Google Colab and AWS instances to run the LLM
    \item \textbf{6.3} Attempted at setting up a docker container on the company server to run the Llama 2 model
    \item \textbf{6.4} Found a \href{https://arxiv.org/pdf/2210.07316.pdf}{Massive Text Embedding Benchmark (MTEB)} model to try
\end{itemize}

\subsection*{6.1 Setting up Llama 2 7B-chat model}
Findings:
\begin{itemize}[topsep=0pt]
    \item Llama 2 comes with some variants, \{7B, 13B, 70B\} models. I was only able to run the 7B model locally due to system requirements. \textcolor{red}{The 13B model works perfectly on the company's server.}
    \item There is still much work to do to fine tune this model, i.e. it still needs to learn how to map MITRE ATT\&CKS to STRIDE.
\end{itemize}


\subsection*{6.2 Google Colab and AWS instance}
\begin{itemize}[topsep=0pt]
    \item Colab is limited by the free storage of 15GB. The smallest 7B model is already 13.48GB and after quantizing it, there is an additional binary file of 3.83GB bringing total storage required to >18GB. Hence, insufficient memory.
    \item AWS instance is doable, but storage costs \$0.023 per GB. The hassle comes when I have to delete all files after each time I done working with the server to avoid incurring a charge.
\end{itemize}

\subsection*{6.3 Setting up a docker container}
There was no success to report and am in the process of learning how to create docker container to run a Python code.